# -*- coding: utf-8 -*-
"""
General Q-Methodology Analysis (Single Dataset) - TPPP Framework & Network Analysis
- Purpose: Q-factor analysis with 'Technology, People, Place, Process' Framework.
- Input: CSV data (Q01~Q24 + Metadata)
- Features:
  1. Factor Extraction & Rotation (PCA/Varimax)
  2. Factor Arrays (Z-scores)
  3. Distinguishing Statements
  4. TPPP Framework Mapping & Analysis
     - Correlation Matrix with P-values (Statistically Significant Loops)
     - Type-based Radar Charts (Structural Perception)
  5. Network Analysis (Visualizing Feedback Loops)
  6. Factor Optimization (Scree Plot & Kaiser Rule)
  7. Enhanced P-Set Profiling (Demographics Integration)
"""

import io
import numpy as np
import pandas as pd
import streamlit as st
import plotly.express as px
import plotly.graph_objects as go
from scipy.stats import spearmanr, pearsonr, norm as normal_dist
import itertools

# ==========================================
# 1. Configuration & Constants
# ==========================================
st.set_page_config(page_title="General Q-Analysis", layout="wide")

RNG_SEED = 42
rng = np.random.default_rng(RNG_SEED)

# 24 Statements provided by the researcher
STATEMENTS = [
    "ë°ì´í„°ì„¼í„°ëŠ” ìž¬ìƒì—ë„ˆì§€ë¥¼ ì‚¬ìš©í•  ë•Œ í™˜ê²½ ì±…ìž„ì„±ì„ ê°–ì¶˜ ì‹œì„¤ë¡œ í‰ê°€ë°›ì„ ìˆ˜ ìžˆë‹¤.", # Q01
    "ë””ì ¤ì´ë‚˜ ê°€ìŠ¤ ë°œì „ê¸°ë¥¼ ë°±ì—… ì „ë ¥ìœ¼ë¡œ ì‚¬ìš©í•  ê²½ìš° í™˜ê²½ì  ìš°ë ¤ê°€ ì œê¸°ë  ìˆ˜ ìžˆë‹¤.", # Q02
    "ë¬¼ ì ˆì•½ì´ë‚˜ ì¹œí™˜ê²½ ëƒ‰ê° ê¸°ìˆ ì˜ ë„ìž…ì€ ì‹œë¯¼ ì‹ ë¢°ì— ê¸ì •ì  ì˜í–¥ì„ ì¤„ ìˆ˜ ìžˆë‹¤.", # Q03
    "ê¸°ìˆ ì´ ìµœì‹ ì´ë”ë¼ë„ ì•ˆì „ì„± í™•ë³´ê°€ ë¶€ì¡±í•˜ë©´ ì‹œë¯¼ ë¶ˆì•ˆì„ ìœ ë°œí•  ìˆ˜ ìžˆë‹¤.", # Q04
    "ë°ì´í„°ì„¼í„° ê¸°ìˆ ì€ ë¹„ìš© íš¨ìœ¨ì„±ë³´ë‹¤ëŠ” ì‚¬íšŒì  ì±…ìž„ì„ ìš°ì„ ì‹œí•´ì•¼ í•œë‹¤ëŠ” ê²¬í•´ê°€ ìžˆë‹¤.", # Q05
    "ê¸°ìˆ ì´ ë‚¯ì„¤ê±°ë‚˜ ë³µìž¡í•˜ê²Œ ì¸ì‹ë˜ë©´ ì‹œë¯¼ê³¼ì˜ ê±°ë¦¬ê°ì´ ì»¤ì§ˆ ìˆ˜ ìžˆë‹¤.", # Q06
    "ë°ì´í„°ì„¼í„° ê±´ì„¤ ê³¼ì •ì— ì‹œë¯¼ ì˜ê²¬ì´ ë°˜ì˜ë˜ì§€ ì•Šìœ¼ë©´ ë°˜ë°œ ê°€ëŠ¥ì„±ì´ ë†’ì•„ì§ˆ ìˆ˜ ìžˆë‹¤.", # Q07
    "ì§€ì—­ ì‚¬íšŒì™€ ìž¥ê¸°ì  ê´€ê³„ë¥¼ ë§ºì–´ì˜¨ ê¸°ì—…ì€ ë” ë†’ì€ ì‹ ë¢°ë¥¼ ë°›ì„ ìˆ˜ ìžˆë‹¤.", # Q08
    "ì„¤ëª…íšŒê°€ í˜•ì‹ì ìœ¼ë¡œ ë³´ì¼ ê²½ìš°, ì‹œë¯¼ ë¶ˆì‹ ì„ ìœ ë°œí•  ìˆ˜ ìžˆë‹¤.", # Q09
    "ì •ë³´ ì ‘ê·¼ì„±ì´ ë‚®ì„ìˆ˜ë¡ ì‹œë¯¼ì˜ ë¶ˆì•ˆê³¼ ì˜ì‹¬ì´ ì¦ê°€í•  ìˆ˜ ìžˆë‹¤.", # Q10
    "ê°ˆë“± ìƒí™©ì—ì„œëŠ” ì¤‘ë¦½ì  ì œ3ìžì˜ ê°œìž…ì´ ì¡°ì •ì— ë„ì›€ì´ ë  ìˆ˜ ìžˆë‹¤.", # Q11
    "ë™ì¼í•œ ì„¤ëª…ì´ë¼ë„ ì •ë¶€ê°€ ì „ë‹¬í•  ê²½ìš° ê¸°ì—…ë³´ë‹¤ ë” ì‹ ë¢°ë°›ì„ ê°€ëŠ¥ì„±ì´ ìžˆë‹¤.", # Q12
    "ê¸°ì¡´ ê³µìž¥ì´ë‚˜ ë°œì „ì†Œ ë¶€ì§€ë¥¼ ìž¬í™œìš©í•œ ë°ì´í„°ì„¼í„°ëŠ” ìˆ˜ìš©ì„±ì´ ë†’ì•„ì§ˆ ìˆ˜ ìžˆë‹¤.", # Q13
    "ì§€ì—­ ì •ì²´ì„±ê³¼ ì¡°í™”ë¥¼ ì´ë£¨ì§€ ëª»í•˜ëŠ” ìž…ì§€ëŠ” ê±°ë¶€ê°ì„ ìœ ë°œí•  ìˆ˜ ìžˆë‹¤.", # Q14
    "ìžì—°ê²½ê´€ í›¼ì†ì´ ë°œìƒí•˜ëŠ” ê²½ìš°, ê¸°ìˆ  ìš°ìˆ˜ì„±ë§Œìœ¼ë¡œ ìˆ˜ìš©ì„± í™•ë³´ëŠ” ì–´ë ¤ìš¸ ìˆ˜ ìžˆë‹¤.", # Q15
    "ìˆ˜ë„ê¶Œê³¼ ì§€ë°©ì€ ë°ì´í„°ì„¼í„° ìž…ì§€ì— ëŒ€í•´ ì„œë¡œ ë‹¤ë¥¸ ê¸°ì¤€ì„ ê°€ì§ˆ ìˆ˜ ìžˆë‹¤.", # Q16
    "ì™¸ë¶€ ìžë³¸ ì£¼ë„ì˜ ì¼ë°©ì ì¸ ìž…ì§€ ê²°ì •ì€ ì§€ì—­ì‚¬íšŒì˜ ì‹ ë¢°ë¥¼ ì €í•´í•  ìˆ˜ ìžˆë‹¤.", # Q17
    "ì§€ì—­ì— ì‹¤ì§ˆì ì¸ í˜œíƒì´ ì œê³µë˜ë©´ ì‹œë¯¼ ìˆ˜ìš©ì„±ì´ ë†’ì•„ì§ˆ ìˆ˜ ìžˆë‹¤.", # Q18
    "ì´ˆê¸° ë‹¨ê³„ì—ì„œ ì •ë³´ê°€ íˆ¬ëª…í•˜ê²Œ ê³µê°œë˜ë©´ ì‹œë¯¼ ì‹ ë¢°ê°€ ë†’ì•„ì§ˆ ìˆ˜ ìžˆë‹¤.", # Q19
    "í™˜ê²½ì˜í–¥í‰ê°€ ê²°ê³¼ëŠ” ì‹œë¯¼ë“¤ì˜ ìˆ˜ìš© ì—¬ë¶€ì— ì¤‘ìš”í•œ íŒë‹¨ ê¸°ì¤€ì´ ë  ìˆ˜ ìžˆë‹¤.", # Q20
    "ê¸°ì—…ê³¼ ì§€ìžì²´ê°€ ê³µë™ìœ¼ë¡œ ê²°ì •í•œ í”„ë¡œì íŠ¸ëŠ” ë” ë†’ì€ ì‹ ë¢°ë¥¼ ì–»ì„ ìˆ˜ ìžˆë‹¤.", # Q21
    "ë²•ì  ìš”ê±´ì„ ì¶©ì¡±í•˜ë”ë¼ë„ ì‹œë¯¼ ì‹ ë¢°ë¥¼ í™•ë³´í•˜ë ¤ë©´ ì¶”ê°€ì ì¸ ì„¤ëª…ì´ í•„ìš”í•  ìˆ˜ ìžˆë‹¤.", # Q22
    "ì§€ì—­ ì–¸ë¡ ì´ ì‹ ì†í•˜ê³  ì •í™•í•˜ê²Œ ì •ë³´ë¥¼ ì „ë‹¬í•˜ë©´ ì‹ ë¢°ì„± ì œê³ ì— ê¸°ì—¬í•  ìˆ˜ ìžˆë‹¤.", # Q23
    "ë°ì´í„°ì„¼í„° ì™„ê³µ ì´í›„ì—ë„ ëª¨ë‹ˆí„°ë§ê³¼ í”¼ë“œë°± ì²´ê³„ê°€ ì§€ì†ë˜ë©´ ì‹ ë¢° ìœ ì§€ì— ë„ì›€ì´ ë  ìˆ˜ ìžˆë‹¤." # Q24
]

# Map Q01 -> Statement[0]
Q_MAP = {f"Q{i+1:02d}": txt for i, txt in enumerate(STATEMENTS)}

# TPPP Mapping (Based on content)
TPPP_CATEGORIES = {
    "Technology": ["Q01", "Q02", "Q03", "Q04", "Q05", "Q06", "Q24"],
    "People (Trust)": ["Q08", "Q09", "Q10", "Q11", "Q12", "Q22", "Q23"],
    "Place (Location)": ["Q13", "Q14", "Q15", "Q16", "Q17", "Q18"],
    "Process (Governance)": ["Q07", "Q19", "Q20", "Q21"]
}

# Reverse mapping for easy lookup
Q_TO_TPPP = {}
for cat, items in TPPP_CATEGORIES.items():
    for item in items:
        Q_TO_TPPP[item] = cat

# ==========================================
# 2. Math & Q-Logic Core
# ==========================================

def standardize_rows(X):
    """Row-wise Z-score normalization"""
    mean = np.nanmean(X, axis=1, keepdims=True)
    std = np.nanstd(X, axis=1, ddof=1, keepdims=True)
    std[std == 0] = 1.0 
    return (X - mean) / std

class QEngine:
    def __init__(self, data_df, n_factors=3, rotation=True):
        self.raw_df = data_df
        self.n_factors = n_factors
        self.rotation = rotation
        
        # Data Cleaning
        self.q_df = data_df.select_dtypes(include=[np.number])
        temp_data = self.q_df.values
        
        # Row-wise Mean Imputation
        row_means = np.nanmean(temp_data, axis=1)
        inds = np.where(np.isnan(temp_data))
        temp_data[inds] = np.take(row_means, inds[0])
        self.data = np.nan_to_num(temp_data, nan=0.0)
        
        self.n_persons, self.n_items = self.data.shape
        self.loadings = None
        self.factor_arrays = None
        self.explained_variance = None
        self.eigenvalues = None
        
    def fit(self):
        # 1. Correlation (Spearman for Likert)
        R, _ = spearmanr(self.data, axis=1)
        z_data = standardize_rows(self.data)
        R = np.nan_to_num(R, nan=0.0)
        
        # 2. Eigen Decomposition
        eigvals, eigvecs = np.linalg.eigh(R)
        idx = eigvals.argsort()[::-1]
        eigvals = eigvals[idx]
        eigvecs = eigvecs[:, idx]
        self.eigenvalues = eigvals
        
        # 3. Extract Factors
        k = self.n_factors
        valid_eigvals = np.maximum(eigvals[:k], 0)
        L = eigvecs[:, :k] * np.sqrt(valid_eigvals)
        
        # 4. Varimax Rotation
        if self.rotation and k > 1:
            L = self._varimax(L)
            
        self.loadings = L
        self.explained_variance = eigvals[:k]
        
        # 5. Factor Arrays (Z-scores)
        self.factor_arrays = self._calculate_factor_arrays(L, z_data)
        
        return self

    def _varimax(self, Phi, gamma=1.0, q=20, tol=1e-6):
        p, k = Phi.shape
        R = np.eye(k)
        d = 0
        for i in range(q):
            d_old = d
            Lambda = np.dot(Phi, R)
            u, s, vh = np.linalg.svd(
                np.dot(Phi.T, (Lambda**3 - (gamma/p) * np.dot(Lambda, np.diag(np.diag(np.dot(Lambda.T, Lambda))))))
            )
            R = np.dot(u, vh)
            d = np.sum(s)
            if d_old != 0 and d/d_old < 1 + tol: break
        return np.dot(Phi, R)

    def _calculate_factor_arrays(self, loadings, z_data):
        n_items = z_data.shape[1]
        arrays = np.zeros((n_items, self.n_factors))
        for f in range(self.n_factors):
            l_vec = loadings[:, f]
            l_clean = np.clip(l_vec, -0.95, 0.95)
            weights = l_clean / (1 - l_clean**2)
            
            w_abs_sum = np.sum(np.abs(weights))
            if w_abs_sum < 1e-6:
                arrays[:, f] = 0
                continue
            
            weighted_sum = np.dot(weights, z_data)
            arr_mean = np.mean(weighted_sum)
            arr_std = np.std(weighted_sum, ddof=1)
            if arr_std == 0: arr_std = 1.0
            arrays[:, f] = (weighted_sum - arr_mean) / arr_std
        return arrays

def strict_respondent_assignment(loadings, threshold=0.4, min_gap=0.1):
    """Assigns respondents to factors strictly."""
    assignments = []
    abs_loads = np.abs(loadings)
    for i in range(len(loadings)):
        row = abs_loads[i]
        max_idx = np.argmax(row)
        max_val = row[max_idx]
        sorted_row = np.sort(row)
        second_max = sorted_row[-2] if len(row) > 1 else 0
        
        if max_val < threshold:
            assignments.append("None (Low)")
        elif (max_val - second_max) < min_gap:
            assignments.append("Confounded")
        else:
            assignments.append(f"Type {max_idx+1}")
    return assignments

def find_distinguishing_items(factor_arrays, n_factors, item_labels, q_map, alpha=0.01):
    """Identifies distinguishing items"""
    col_names = [f"F{i+1}" for i in range(n_factors)]
    df_arrays = pd.DataFrame(factor_arrays, columns=col_names, index=item_labels)
    crit_z = normal_dist.ppf(1 - alpha/2)
    se = 0.3 

    dist_dict = {}
    for i in range(n_factors):
        target_col = f"F{i+1}"
        other_cols = [c for c in df_arrays.columns if c != target_col]
        if not other_cols: continue
        
        min_diff_val = pd.Series(np.inf, index=df_arrays.index)
        is_significant = pd.Series(True, index=df_arrays.index)
        
        for other in other_cols:
            diff = df_arrays[target_col] - df_arrays[other]
            z_stat = diff / (np.sqrt(2) * se)
            sig_check = (np.abs(z_stat) > crit_z)
            is_significant &= sig_check
            update_mask = np.abs(diff) < np.abs(min_diff_val)
            min_diff_val[update_mask] = diff[update_mask]
            
        dist_items = df_arrays[is_significant].copy()
        if not dist_items.empty:
            dist_items['Min Difference'] = min_diff_val[is_significant]
            dist_items['Direction'] = np.where(dist_items['Min Difference'] > 0, 'Higher', 'Lower')
            dist_items['Statement'] = [q_map.get(idx, "") for idx in dist_items.index]
            dist_items = dist_items.sort_values('Min Difference', ascending=False, key=abs)
            
            # Add TPPP Category
            dist_items['Category'] = [Q_TO_TPPP.get(idx, "Unknown") for idx in dist_items.index]
            
            cols = ['Category', 'Statement', 'Min Difference', 'Direction'] + col_names
            dist_dict[target_col] = dist_items[cols]
    return dist_dict

def calculate_tppp_scores(df_q, mapping):
    """Calculates average scores for TPPP categories per respondent"""
    scores = pd.DataFrame(index=df_q.index)
    for cat, items in mapping.items():
        # Only use items present in df_q
        valid_items = [i for i in items if i in df_q.columns]
        if valid_items:
            scores[cat] = df_q[valid_items].mean(axis=1)
    return scores

def calculate_type_tppp_profile(factor_arrays, q_labels, mapping):
    """Calculates average Z-score for TPPP categories per Factor Type"""
    df_arrays = pd.DataFrame(factor_arrays, index=q_labels)
    n_factors = df_arrays.shape[1]
    
    profiles = {}
    for i in range(n_factors):
        f_name = f"F{i+1}"
        cat_scores = {}
        for cat, items in mapping.items():
            valid_items = [item for item in items if item in df_arrays.index]
            if valid_items:
                cat_scores[cat] = df_arrays.loc[valid_items, i].mean()
        profiles[f_name] = cat_scores
        
    return pd.DataFrame(profiles)

def create_network_graph(corr_matrix, p_matrix, threshold=0.3, sig_level=0.05):
    """
    Creates a Network Graph for TPPP Feedback Loops using Plotly.
    Only draws edges if |correlation| > threshold AND p-value < sig_level.
    """
    
    nodes = list(corr_matrix.columns)
    # Positions: Tech(Top), People(Right), Place(Bottom), Process(Left)
    pos = {
        nodes[0]: (0, 1),   # Tech
        nodes[1]: (1, 0),   # People
        nodes[2]: (0, -1),  # Place
        nodes[3]: (-1, 0)   # Process
    }
    
    edge_x = []
    edge_y = []
    edge_text = []
    edge_colors = []
    
    # Add Edges
    for i in range(len(nodes)):
        for j in range(i+1, len(nodes)):
            n1, n2 = nodes[i], nodes[j]
            corr_val = corr_matrix.iloc[i, j]
            p_val = p_matrix.iloc[i, j]
            
            # Check significance & threshold
            if abs(corr_val) >= threshold and p_val < sig_level:
                x0, y0 = pos[n1]
                x1, y1 = pos[n2]
                
                # Create individual edge trace for varying width/color? 
                # For simplicity in one trace, we can't vary width easily in go.Scatter lines mode.
                # We will just draw lines. Color can be based on sign.
                
                edge_x.extend([x0, x1, None])
                edge_y.extend([y0, y1, None])
                
                sign = "(+)" if corr_val > 0 else "(-)"
                sig_mark = "**" if p_val < 0.01 else "*"
                edge_text.append(f"{n1}â†”{n2}<br>r={corr_val:.2f} {sign}<br>p={p_val:.3f}{sig_mark}")

    # Edge Trace
    edge_trace = go.Scatter(
        x=edge_x, y=edge_y,
        line=dict(width=3, color='#555'), # Uniform color for simplicity
        hoverinfo='text',
        text=str(edge_text), # Tooltip issues with single string list, improved below
        mode='lines')
    
    # Hack for hover text on lines (Plotly limitation): 
    # Usually requires defining middle points. Skipping complex impl for stability.
    
    # Node Trace
    node_x = [pos[n][0] for n in nodes]
    node_y = [pos[n][1] for n in nodes]
    
    node_trace = go.Scatter(
        x=node_x, y=node_y,
        mode='markers+text',
        text=nodes,
        textposition="top center",
        hoverinfo='text',
        marker=dict(
            showscale=False,
            color='#1f77b4',
            size=40,
            line_width=2))

    fig = go.Figure(data=[edge_trace, node_trace],
                 layout=go.Layout(
                    title=f'TPPP Feedback Loops (r > {threshold}, p < {sig_level})',
                    title_font_size=16,
                    showlegend=False,
                    hovermode='closest',
                    margin=dict(b=20,l=5,r=5,t=40),
                    xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                    yaxis=dict(showgrid=False, zeroline=False, showticklabels=False))
                )
    return fig

def detect_strongest_loops(corr_matrix):
    """Detects 3-node feedback loops based on correlation strength"""
    cols = corr_matrix.columns.tolist()
    triads = []
    
    # Iterate all combinations of 3
    for triad in itertools.combinations(cols, 3):
        a, b, c = triad
        # Sum of correlations (Strength of loop)
        score = abs(corr_matrix.loc[a, b]) + abs(corr_matrix.loc[b, c]) + abs(corr_matrix.loc[c, a])
        avg_score = score / 3
        triads.append({
            "Loop": f"{a} â†” {b} â†” {c}",
            "Strength (Avg Corr)": avg_score,
            "Links": [f"{corr_matrix.loc[a,b]:.2f}", f"{corr_matrix.loc[b,c]:.2f}", f"{corr_matrix.loc[c,a]:.2f}"]
        })
    
    return pd.DataFrame(triads).sort_values("Strength (Avg Corr)", ascending=False)

def plot_scree(eigenvalues):
    """Plots Scree Plot for Factor Selection"""
    x = range(1, len(eigenvalues) + 1)
    
    fig = go.Figure()
    fig.add_trace(go.Scatter(x=list(x), y=eigenvalues, mode='lines+markers', name='Eigenvalue'))
    fig.add_hline(y=1.0, line_dash="dash", line_color="red", annotation_text="Kaiser Criterion (1.0)")
    
    fig.update_layout(
        title="Scree Plot (Eigenvalues)",
        xaxis_title="Factor Number",
        yaxis_title="Eigenvalue",
        template="plotly_white"
    )
    return fig

def calculate_corr_with_pvalues(df):
    """Calculates correlation matrix and p-values matrix"""
    df = df.dropna()
    cols = df.columns
    corr_mat = pd.DataFrame(index=cols, columns=cols, dtype=float)
    p_mat = pd.DataFrame(index=cols, columns=cols, dtype=float)
    
    for r in cols:
        for c in cols:
            if r == c:
                corr_mat.loc[r, c] = 1.0
                p_mat.loc[r, c] = 0.0
            else:
                # Spearman is standard for Likert
                corr, p = spearmanr(df[r], df[c])
                corr_mat.loc[r, c] = corr
                p_mat.loc[r, c] = p
                
    return corr_mat, p_mat

# ==========================================
# 3. Main UI
# ==========================================

st.title("ðŸ“Š General Q-Analysis: TPPP Framework")
st.markdown("### TPPP (Technology, People, Place, Process) ì¤‘ì‹¬ ë¶„ì„")

# File Upload
uploaded_file = st.sidebar.file_uploader("Upload Responses CSV", type=['csv'])

if uploaded_file:
    try:
        df_raw = pd.read_csv(uploaded_file)
        q_cols = [c for c in df_raw.columns if c.startswith('Q') and c[1:].isdigit() and int(c[1:]) <= 24]
        
        if len(q_cols) < 5:
            st.error("ë°ì´í„°ì—ì„œ Q01~Q24 ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            st.stop()
            
        df_q = df_raw[q_cols]
        id_col = next((c for c in df_raw.columns if 'email' in c.lower()), None)
        ids = df_raw[id_col] if id_col else [f"P{i}" for i in range(len(df_raw))]
        meta_cols = [c for c in df_raw.columns if c not in q_cols and c != id_col]
        
        st.sidebar.success(f"Loaded {len(df_raw)} respondents.")
        
    except Exception as e:
        st.error(f"File Load Error: {e}")
        st.stop()

    with st.sidebar:
        st.header("Analysis Settings")
        n_factors = st.number_input("Number of Factors", 1, 10, 3)
        assign_thr = st.slider("Assignment Threshold (>)", 0.3, 0.7, 0.4, 0.05)
        assign_gap = st.slider("Confounded Gap (>)", 0.05, 0.3, 0.1, 0.05)

    # Run Engine
    engine = QEngine(df_q, n_factors=n_factors).fit()
    assignments = strict_respondent_assignment(engine.loadings, threshold=assign_thr, min_gap=assign_gap)
    
    # Calculate scores for TPPP analysis
    tppp_scores = calculate_tppp_scores(df_q, TPPP_CATEGORIES)
    # [UPDATE] Calc Correlation AND P-values
    corr_matrix, p_matrix = calculate_corr_with_pvalues(tppp_scores)

    # Prepare Metadata for P-Set Analysis
    df_meta = df_raw[meta_cols].copy()
    df_meta['Assigned Type'] = assignments
    df_meta['ID'] = ids

    # Tabs
    tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs([
        "1. Factor Structure", 
        "2. TPPP Profiles (Radar)", 
        "3. TPPP Network & Loops",
        "4. Distinguishing Statements",
        "5. Raw Data & Arrays",
        "6. P-Set Profiling (Demographics)"
    ])
    
    # --- Tab 1: Structure ---
    with tab1:
        st.header("1. Factor Optimization & Structure")
        
        # Factor Optimization Section
        col_opt1, col_opt2 = st.columns([2, 1])
        with col_opt1:
            st.subheader("Optimal Factors (Scree Plot & Kaiser)")
            st.plotly_chart(plot_scree(engine.eigenvalues[:10]), use_container_width=True)
        
        with col_opt2:
            st.markdown("<br><br>", unsafe_allow_html=True) # Spacer
            kaiser_k = sum(engine.eigenvalues > 1.0)
            st.metric("Kaiser Criterion (k)", f"{kaiser_k} Factors", "Eigenvalue > 1.0")
            st.info(f"ë°ì´í„° í†µê³„ìƒ **{kaiser_k}ê°œ ìš”ì¸**ì´ ê¶Œìž¥ë©ë‹ˆë‹¤. (ê·¸ëž˜í”„ì˜ êº¾ìž„ìƒˆë¥¼ í™•ì¸í•˜ì„¸ìš”)")

        st.divider()

        st.subheader("Respondents by Type")
        counts = pd.Series(assignments).value_counts().sort_index()
        c1, c2 = st.columns([1, 2])
        with c1:
            st.dataframe(pd.DataFrame({"Count": counts, "Ratio": (counts/len(assignments)*100).apply(lambda x: f"{x:.1f}%")}))
        with c2:
            st.bar_chart(counts)
        
        st.subheader("Factor Interpretation (Top Items)")
        # Show top 3 agreement/disagreement items per factor
        fa_df = pd.DataFrame(engine.factor_arrays, index=q_cols, columns=[f"F{i+1}" for i in range(n_factors)])
        
        cols = st.columns(n_factors)
        for i, col in enumerate(cols):
            f_key = f"F{i+1}"
            with col:
                st.markdown(f"**{f_key} Top/Bottom**")
                sorted_f = fa_df[f_key].sort_values(ascending=False)
                top3 = sorted_f.head(3)
                bot3 = sorted_f.tail(3)
                
                st.markdown("ðŸ‘ **Strong Agreement**")
                for idx, val in top3.items():
                    st.caption(f"**{idx}** ({Q_TO_TPPP.get(idx)}): {Q_MAP[idx][:30]}... (z={val:.2f})")
                
                st.markdown("ðŸ‘Ž **Strong Disagreement**")
                for idx, val in bot3.items():
                    st.caption(f"**{idx}** ({Q_TO_TPPP.get(idx)}): {Q_MAP[idx][:30]}... (z={val:.2f})")

    # --- Tab 2: TPPP Profiles ---
    with tab2:
        st.header("TPPP Perception Profiles (Radar Chart)")
        type_profiles = calculate_type_tppp_profile(engine.factor_arrays, q_cols, TPPP_CATEGORIES)
        
        categories = list(type_profiles.index)
        fig = go.Figure()

        for col in type_profiles.columns:
            fig.add_trace(go.Scatterpolar(
                r=type_profiles[col],
                theta=categories,
                fill='toself',
                name=col
            ))

        fig.update_layout(
            polar=dict(
                radialaxis=dict(visible=True, range=[-1.5, 1.5]) 
            ),
            showlegend=True,
            title="Type-specific TPPP Weighting (Z-scores)"
        )
        st.plotly_chart(fig, use_container_width=True)
        st.dataframe(type_profiles.style.background_gradient(cmap="RdBu_r", vmin=-1, vmax=1).format("{:.3f}"))

    # --- Tab 3: TPPP Network & Loops (NEW) ---
    with tab3:
        st.header("TPPP Network Analysis & Feedback Loops")
        st.markdown("4ê°œ ì°¨ì› ê°„ì˜ **ìƒí˜¸ìž‘ìš©(Correlation)**ê³¼ **ìˆœí™˜ ê³ ë¦¬(Feedback Loop)**ë¥¼ íƒìƒ‰í•©ë‹ˆë‹¤.")
        
        c1, c2 = st.columns([1, 2])
        with c1:
            st.subheader("Settings")
            net_threshold = st.slider("Correlation Threshold (|r| >)", 0.0, 0.8, 0.3, 0.05)
            sig_alpha = st.selectbox("Significance Level (p <)", [0.01, 0.05, 0.10], index=1)
            
            st.subheader("Correlation Matrix (Spearman)")
            st.dataframe(corr_matrix.style.background_gradient(cmap="coolwarm", vmin=-1, vmax=1).format("{:.3f}"))
            
            st.subheader("P-Values")
            st.dataframe(p_mat = p_matrix.style.applymap(lambda x: 'color: red' if x < sig_alpha else 'color: black').format("{:.4f}"))
            
            st.subheader("Strongest Loop Detection (Triads)")
            loops_df = detect_strongest_loops(corr_matrix)
            st.dataframe(loops_df.style.format({"Strength (Avg Corr)": "{:.3f}"}))

        with c2:
            st.subheader("Network Visualization (Significant Links Only)")
            fig_net = create_network_graph(corr_matrix, p_matrix, net_threshold, sig_alpha)
            st.plotly_chart(fig_net, use_container_width=True)
            st.info("""
            **í•´ì„ ê°€ì´ë“œ:**
            * **ì—°ê²°ì„ (Link):** ë‘ ìš”ì†Œê°€ í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•˜ê²Œ(p < alpha) ê°•í•˜ê²Œ ì—°ê²°ëœ ê²½ìš°ë§Œ í‘œì‹œë©ë‹ˆë‹¤.
            * **í”¼ë“œë°± ë£¨í”„:** ì‚¼ê°í˜• í˜•íƒœì˜ ì—°ê²°ì€ ì„¸ ìš”ì†Œê°€ ì„œë¡œ ì˜í–¥ì„ ì£¼ê³ ë°›ìœ¼ë©° ì¸ì‹ì„ ê°•í™”í•˜ëŠ” í•µì‹¬ êµ¬ì¡°ìž…ë‹ˆë‹¤.
            """)

    # --- Tab 4: Distinguishing ---
    with tab4:
        st.subheader("Distinguishing Statements per Type")
        dist_dict = find_distinguishing_items(engine.factor_arrays, n_factors, q_cols, Q_MAP, alpha=0.05)
        
        d_tabs = st.tabs([f"Factor {i+1}" for i in range(n_factors)])
        for i, tab in enumerate(d_tabs):
            with tab:
                f_key = f"F{i+1}"
                res = dist_dict.get(f_key)
                if res is not None:
                    st.dataframe(
                        res.style.background_gradient(cmap="coolwarm", subset=["Min Difference"], vmin=-2, vmax=2)
                        .format({"Min Difference": "{:.2f}"}),
                        use_container_width=True
                    )
                else:
                    st.info("ì´ ìš”ì¸ì„ êµ¬ë³„í•˜ëŠ” ë¬¸í•­ì´ ì—†ìŠµë‹ˆë‹¤.")

    # --- Tab 5: Raw Data ---
    with tab5:
        st.subheader("Factor Arrays (All Items)")
        fa_df = pd.DataFrame(engine.factor_arrays, index=q_cols, columns=[f"F{i+1}" for i in range(n_factors)])
        fa_df.insert(0, "Category", [Q_TO_TPPP.get(idx) for idx in fa_df.index])
        fa_df.insert(1, "Statement", [Q_MAP.get(idx) for idx in fa_df.index])
        st.dataframe(fa_df.style.background_gradient(cmap="RdBu_r", subset=[f"F{i+1}" for i in range(n_factors)]))

    # --- Tab 6: P-Set Profiling (NEW) ---
    with tab6:
        st.header("P-Set Profiling (Demographics Integration)")
        st.markdown("ê° ìœ í˜•(Type)ì— ì†í•œ ì‘ë‹µìžë“¤ì˜ **ì¸êµ¬í†µê³„í•™ì  íŠ¹ì„±**ì„ êµì°¨ ë¶„ì„í•©ë‹ˆë‹¤.")
        
        if not meta_cols:
            st.warning("ë°ì´í„°ì…‹ì— ë©”íƒ€ë°ì´í„° ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        else:
            selected_meta = st.selectbox("Select Demographic Variable:", meta_cols)
            
            # Check if numeric (like years) or categorical
            is_numeric_meta = pd.to_numeric(df_meta[selected_meta], errors='coerce').notna().all()
            
            if is_numeric_meta:
                # Group by mean
                st.subheader(f"Average {selected_meta} by Type")
                df_meta_num = df_meta.copy()
                df_meta_num[selected_meta] = pd.to_numeric(df_meta_num[selected_meta])
                
                avg_stats = df_meta_num.groupby('Assigned Type')[selected_meta].mean().sort_index()
                st.bar_chart(avg_stats)
                st.dataframe(avg_stats)
            else:
                # Cross-tabulation Heatmap
                st.subheader(f"Distribution of {selected_meta} by Type")
                ctab = pd.crosstab(df_meta['Assigned Type'], df_meta[selected_meta])
                
                # Plotly Heatmap
                fig_heat = px.imshow(ctab, text_auto=True, aspect="auto", 
                                   color_continuous_scale="Greens",
                                   title=f"Heatmap: Type vs {selected_meta}")
                st.plotly_chart(fig_heat, use_container_width=True)
                
                # Normalized (Row %)
                st.caption("Row Percentage (Type Composition)")
                ctab_norm = pd.crosstab(df_meta['Assigned Type'], df_meta[selected_meta], normalize='index') * 100
                st.dataframe(ctab_norm.style.format("{:.1f}%").background_gradient(cmap="Greens", axis=1))

else:
    st.info("ì¢Œì¸¡ ì‚¬ì´ë“œë°”ì—ì„œ CSV íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
